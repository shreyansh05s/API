# API
Audio Processing and Indexing Tool

## How to run

### Pre-requisites
We assume ```docker``` and ```docker-compose``` to be available on your system, if not then they can be installed using the package manager of your choice for your platform.

Open a terminal on the ```src``` folder of this project and run the following command
   ``` docker-compose up```
Wait for a couple of minutes for all services to run 

The UI is available on ```0.0.0.0:8501```

## Indexing
For indexing we are making use of the Opensearch engine for effecient index management in a machine learing environment. In this environment we are performing 2 types of indexing of two different data which are internally linked for a better kNN classification for genre classifcation

1. **Embedding:** With the embeddings generated by the PANN-based embedding generation for downstream task usage, the data is stored in an index for genre classification. This is also linked to the labels generated from the downstream task.

2. **Tagging:** The labels generated from the model are linked to the embeddings generated in the above task and stored in an index. This allows us to retrieve information regarding an audio clip effeciently even on very large data stored on the OpenSearch.

## Tagging
For tagging we make use of the [ConvNeXT](https://github.com/topel/audioset-convnext-inf/tree/main) model to generate the inference and the embneddings for the audio files being used
Any '.wav' can be used for the input, the processing works best for a 10s clip of the audio ( which is done internally).

[AudioSet](https://research.google.com/audioset/) is the dataset used during training and testing of the model.
Fine tuning can be done through additional dataset augmentation but we haev gone with the default dataset due to its size and coverage in annotation.



