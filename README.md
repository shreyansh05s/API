# API
Audio Processing and Indexing Tool

## Indexing
For indexing we are making use of the Opensearch engine for effecient index management in a machine learing environment. In this environment we are performing 2 types of indexing of two different data which are internally linked for a better kNN classification for genre classifcation

1. **Embedding:** With the embeddings generated by the PANN-based embedding generation for downstream task usage, the data is stored in an index for genre classification. This is also linked to the labels generated from the downstream task.

2. **Tagging:** The labels generated from the model are linked to the embeddings generated in the above task and stored in an index. This allows us to retrieve information regarding an audio clip effeciently even on very large data stored on the OpenSearch.

## Tagging
For tagging we make use of the [ConvNeXT](https://github.com/topel/audioset-convnext-inf/tree/main) model to generate the inference and the embneddings for the audio files being used
Any '.wav' can be used for the input, the processing works best for a 10s clip of the audio ( which is done internally).

[AudioSet](https://research.google.com/audioset/) is the dataset used during training and testing of the model.
Fine tuning can be done through additional dataset augmentation but we haev gone with the default dataset due to its size and coverage in annotation.

To run the tagging service, make sure to install the pre-requisites through a pip install
```
pip install git+https://github.com/topel/audioset-convnext-inf@pip-install
```

To run the tagging servive, execute the below command
```
python tagging.py
```

